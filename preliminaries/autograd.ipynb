{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0f2115-dc19-4235-91ad-61d8412616ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af68f25-cca9-43d6-af0e-24c842696c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "x.grad  # The gradient is None by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb05a79a-2f31-4dd0-b585-1952df6ede77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307b5d72-22b5-4770-80ca-a6873ca022c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5013b4-8053-471e-9601-4afa94a6e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17dd03eb-3b87-4c67-8a18-da25f48ac434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()  # Reset the gradient\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c92541-aeb9-442f-b85d-4ce7ebc91dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# y.backward(gradient=torch.ones(len(y)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fa873ab-202b-4582-a29f-9073d093ed35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a86b53a3-c439-46ea-aab2-309855b3986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "class CustomSGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        # 初始化，设置学习率\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"执行单次优化步骤：参数 = 参数 - 学习率 * 梯度\"\"\"\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            for param in group['params']:\n",
    "                if param.grad is not None:\n",
    "                    # 核心更新逻辑：利用 PyTorch 的原地操作 add_()\n",
    "                    param.data.add_(param.grad, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47ee0e9-cd06-43d4-877e-c538f998a6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 梯度下降过程 ---\n",
      "初始 x: -5.0000, 损失 f(x): 50.0000\n",
      "\n",
      "迭代  1 | x: -3.6000 | 损失 L: 50.0000 | 梯度 dL/dx: -14.0000\n",
      "迭代  2 | x: -2.4800 | 损失 L: 32.3600 | 梯度 dL/dx: -11.2000\n",
      "迭代  3 | x: -1.5840 | 损失 L: 21.0704 | 梯度 dL/dx: -8.9600\n",
      "迭代  4 | x: -0.8672 | 损失 L: 13.8451 | 梯度 dL/dx: -7.1680\n",
      "迭代  5 | x: -0.2938 | 损失 L: 9.2208 | 梯度 dL/dx: -5.7344\n",
      "迭代  6 | x: 0.1650 | 损失 L: 6.2613 | 梯度 dL/dx: -4.5875\n",
      "迭代  7 | x: 0.5320 | 损失 L: 4.3673 | 梯度 dL/dx: -3.6700\n",
      "迭代  8 | x: 0.8256 | 损失 L: 3.1550 | 梯度 dL/dx: -2.9360\n",
      "迭代  9 | x: 1.0605 | 损失 L: 2.3792 | 梯度 dL/dx: -2.3488\n",
      "迭代 10 | x: 1.2484 | 损失 L: 1.8827 | 梯度 dL/dx: -1.8790\n",
      "迭代 11 | x: 1.3987 | 损失 L: 1.5649 | 梯度 dL/dx: -1.5032\n",
      "迭代 12 | x: 1.5190 | 损失 L: 1.3616 | 梯度 dL/dx: -1.2026\n",
      "迭代 13 | x: 1.6152 | 损失 L: 1.2314 | 梯度 dL/dx: -0.9621\n",
      "迭代 14 | x: 1.6921 | 损失 L: 1.1481 | 梯度 dL/dx: -0.7697\n",
      "迭代 15 | x: 1.7537 | 损失 L: 1.0948 | 梯度 dL/dx: -0.6157\n",
      "迭代 16 | x: 1.8030 | 损失 L: 1.0607 | 梯度 dL/dx: -0.4926\n",
      "迭代 17 | x: 1.8424 | 损失 L: 1.0388 | 梯度 dL/dx: -0.3941\n",
      "迭代 18 | x: 1.8739 | 损失 L: 1.0248 | 梯度 dL/dx: -0.3153\n",
      "迭代 19 | x: 1.8991 | 损失 L: 1.0159 | 梯度 dL/dx: -0.2522\n",
      "迭代 20 | x: 1.9193 | 损失 L: 1.0102 | 梯度 dL/dx: -0.2018\n",
      "迭代 21 | x: 1.9354 | 损失 L: 1.0065 | 梯度 dL/dx: -0.1614\n",
      "迭代 22 | x: 1.9483 | 损失 L: 1.0042 | 梯度 dL/dx: -0.1291\n",
      "迭代 23 | x: 1.9587 | 损失 L: 1.0027 | 梯度 dL/dx: -0.1033\n",
      "迭代 24 | x: 1.9669 | 损失 L: 1.0017 | 梯度 dL/dx: -0.0826\n",
      "迭代 25 | x: 1.9736 | 损失 L: 1.0011 | 梯度 dL/dx: -0.0661\n",
      "迭代 26 | x: 1.9788 | 损失 L: 1.0007 | 梯度 dL/dx: -0.0529\n",
      "迭代 27 | x: 1.9831 | 损失 L: 1.0004 | 梯度 dL/dx: -0.0423\n",
      "迭代 28 | x: 1.9865 | 损失 L: 1.0003 | 梯度 dL/dx: -0.0338\n",
      "迭代 29 | x: 1.9892 | 损失 L: 1.0002 | 梯度 dL/dx: -0.0271\n",
      "迭代 30 | x: 1.9913 | 损失 L: 1.0001 | 梯度 dL/dx: -0.0217\n",
      "迭代 31 | x: 1.9931 | 损失 L: 1.0001 | 梯度 dL/dx: -0.0173\n",
      "迭代 32 | x: 1.9945 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0139\n",
      "迭代 33 | x: 1.9956 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0111\n",
      "迭代 34 | x: 1.9965 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0089\n",
      "迭代 35 | x: 1.9972 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0071\n",
      "迭代 36 | x: 1.9977 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0057\n",
      "迭代 37 | x: 1.9982 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0045\n",
      "迭代 38 | x: 1.9985 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0036\n",
      "迭代 39 | x: 1.9988 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0029\n",
      "迭代 40 | x: 1.9991 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0023\n",
      "迭代 41 | x: 1.9993 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0019\n",
      "迭代 42 | x: 1.9994 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0015\n",
      "迭代 43 | x: 1.9995 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0012\n",
      "迭代 44 | x: 1.9996 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0010\n",
      "迭代 45 | x: 1.9997 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0008\n",
      "迭代 46 | x: 1.9998 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0006\n",
      "迭代 47 | x: 1.9998 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0005\n",
      "迭代 48 | x: 1.9998 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0004\n",
      "迭代 49 | x: 1.9999 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0003\n",
      "迭代 50 | x: 1.9999 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0002\n",
      "迭代 51 | x: 1.9999 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0002\n",
      "迭代 52 | x: 1.9999 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0002\n",
      "迭代 53 | x: 1.9999 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0001\n",
      "迭代 54 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0001\n",
      "迭代 55 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0001\n",
      "迭代 56 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0001\n",
      "迭代 57 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0001\n",
      "迭代 58 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 59 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 60 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 61 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 62 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 63 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 64 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 65 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 66 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 67 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 68 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 69 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 70 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 71 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 72 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 73 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 74 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 75 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 76 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 77 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 78 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 79 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 80 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 81 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 82 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 83 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 84 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 85 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 86 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 87 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 88 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 89 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 90 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 91 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 92 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 93 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 94 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 95 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 96 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 97 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 98 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 99 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "迭代 100 | x: 2.0000 | 损失 L: 1.0000 | 梯度 dL/dx: -0.0000\n",
      "\n",
      "--- 最终结果 ---\n",
      "理论最优解 x*: 2.0\n",
      "模型找到的 x: 2.0000\n",
      "最终损失 L: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 初始化参数和优化器 ---\n",
    "\n",
    "# 定义参数 x，并设置 requires_grad=True 启用梯度追踪\n",
    "x = torch.tensor([-5.0], requires_grad=True) # 初始值 x=5.0\n",
    "lr = 0.1 # 学习率\n",
    "\n",
    "# 使用我们自己写的优化器\n",
    "optimizer = CustomSGD([x], lr=lr) \n",
    "\n",
    "# --- 2. 梯度下降循环 ---\n",
    "\n",
    "print(\"--- 梯度下降过程 ---\")\n",
    "print(f\"初始 x: {x.item():.4f}, 损失 f(x): {(x**2 - 4*x + 5).item():.4f}\\n\")\n",
    "\n",
    "num_epochs = 100 # 迭代次数\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # 步骤 A: 前向传播，计算损失 L = f(x)\n",
    "    L = x**2 - 4*x + 5\n",
    "    \n",
    "    # 步骤 B: 梯度清零\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # 步骤 C: 反向传播，计算梯度 dL/dx\n",
    "    L.backward()\n",
    "    \n",
    "    # 步骤 D: 更新参数 (使用我们自定义的 CustomSGD.step())\n",
    "    optimizer.step() \n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"迭代 {epoch+1:2d} | x: {x.item():.4f} | 损失 L: {L.item():.4f} | 梯度 dL/dx: {x.grad.item():.4f}\")\n",
    "\n",
    "# --- 3. 最终结果 ---\n",
    "print(\"\\n--- 最终结果 ---\")\n",
    "print(f\"理论最优解 x*: 2.0\")\n",
    "print(f\"模型找到的 x: {x.item():.4f}\")\n",
    "print(f\"最终损失 L: {(x**2 - 4*x + 5).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e06e861-0277-4f38-a5c9-ad38310c4fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b106e057-1db5-4cbf-bb39-f53e46296b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])\n",
      "tensor(9.)\n"
     ]
    }
   ],
   "source": [
    "print(x[:-1])\n",
    "print(x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994df63-d96f-4ea1-8799-d312f0e91acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
